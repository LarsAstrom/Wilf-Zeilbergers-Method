\section{Polynomials}
In the thesis we are continuously discussing polynomials. We are always using polynomials with integer coefficients -- which we also generalize to include rational coefficients by looking at fractions of polynomials with integer coefficients. Therefore, from now on whenever we talk about polynomials they are assumed to have integer coefficients as long as otherwise is not stated.

We will now have a discussion on what operations we want to be able to perform on the polynomials both theoreticly and partly implementation wise. Therefore there will not be a specific subchapter regarding polynomials later on in chapter \ref{Ch: Implementation}. But first we need to describe how a polynomial can be implemented and stored in the code -- and this is how it was implemented in this thesis.

A polynomial, say $p(n)$, can be seen as a list of coefficients $[a_0,a_1,\ldots,a_d]$. This represents the polynomial
\begin{equation}\label{Eq: Theory,polynomials,general polynomial}
  p(n) = a_0+a_1 n + \ldots + a_d n^d.
\end{equation}
This is fairly simple if the polynomial only has one variable. As soon as we introduce polynomials with more than one variable -- which is the case in all examples where Wilf-Zeilbergers method is used -- it gets a bit more complicated. The way we will visualize polynomials of more than one variable is to try to keep the very same structure as in \ref{Eq: Theory,polynomials,general polynomial}, namely we store a polynomial $p(n)$ as a list of coefficients $[a_0,a_1,\ldots,a_d]$, but instead of having $a_i$ just representing integers they are instead themselves polynomials. In order to show how this can be done, lets show an example.
\begin{example}\label{Ex: polynomial, recursive}
  Lets look at the polynomial
  \begin{equation*}
    \begin{split}
      p(k,m,n) = &1+3n+2n^2-m-2mn-2mn^2+3m^2+3m^2n+3m^2n^2+\\
      &3k-3kn+2kn^2+km-2kmn-km^2+2k^2-k^2n-k^2n^2+\\
      &3k^2m-3k^2mn-2k^2mn^2-3k^2m^2-k^2m^2n-3k^2m^2n^2.
    \end{split}
  \end{equation*}
  This will be stored as
  \begin{equation*}
    \begin{split}
      p(k,m,n) = & -((3k^2+3k-3)m^2-(2k+1)m-(3k^2-k+3))n^2+\\
      &((k^2-1)m^2-(k^2-2k+2)m-(2k+2))n+\\
      &((2k^2+2)m^2+(2k-1)m+(k^2-k-3))
    \end{split}
  \end{equation*}
  Note that this can be written as
  \begin{equation*}
    p(k,m,n) = p_2(k,m)n^2+p_1(k,m)n+p_0,
  \end{equation*}
  where
  \begin{equation*}
    \begin{split}
      p_0(k,m) &= ((2k^2+2)m^2+(2k-1)m+(k^2-k-3))\\
      p_1(k,m) &= ((k^2-1)m^2-(k^2-2k+2)m-(2k+2))\\
      p_2(k,m) &= -((3k^2+3k-3)m^2-(2k+1)m-(3k^2-k+3)).
    \end{split}
  \end{equation*}
  Now since our definition of polynomials is recursive, each of $p_0(k,m),p_1(k,m),p_2(k,m)$ is written as $q_2(k)m^2+q_1(k)m+q_0(k)$ for polynomials $q_0,q_1,q_2$. For instance for $p_1$ we have
  \begin{equation*}
    p_1(k,m) = q_2(k)m^2+q_1(k)m+q_0(k),
  \end{equation*}
  where
  \begin{equation*}
    \begin{split}
      q_0(k) & = -(2k+2) \\
      q_1(k) & = -(k^2-2k+2) \\
      q_2(k) & = (k^2-1)
    \end{split}
  \end{equation*}
  Lastly we have the polynomials $q_0,q_1,q_2$ which have integers as coefficients.
\end{example}
As we saw in example \ref{Ex: polynomial, recursive} we define all polynomials as in equation \ref{Eq: Theory,polynomials,general polynomial} where the coefficients $a_i$ are polynomials or integers, where it is only integers in the base case.

Note that in this case we wrote the polynomial with $n$ ''outmost'' in the representation of $p$. There was no specific reason for this, and the choice is not very important when only looking at one polynomial at a time. When we are using several polynomials at the same time, for instance if we want to add two polynomials, the structure is crucial. Therefore we will define some concepts regarding the representation of a polynomial.

\begin{definition}
  A polynomial $p$ is said to have the variable order $\vec{x}=(x_1,x_2,\ldots,x_m)$ if it is stored as
  \begin{equation}
    p(\vec{x}) = \sum_{i_1=0}^{d_1}\Bigg(\sum_{i_2=0}^{d_2}\Big(\sum_{i_3=0}^{d_3}\ldots\Big)x_2^{i_2}\Bigg)x_1^{i_1}.
  \end{equation}
  Note that the same polynomial can be expressed in various different ways, and we say that two variable orders, $\vec{x}$ and $\vec{y}$, are different if $|\vec{x}|\neq|\vec{y}|$ (where $|\vec{z}|$ denotes the length of the vector $\vec{z}$) or there exist an integer $i$ such that $x_i\neq y_i$.
\end{definition}
\begin{remark}
  Note that if the polynomial $p$ has variable order $\vec{x}$ such that $|\vec{x}|=1$, then the coefficients of $p$ are integers.
\end{remark}

Here we can note that $d_i$ is the degree of $p$ with respect to $x_i$. Furthermore we note that we can express a polynomial $p$ with the variable order $\vec{x}=(x_1,\ldots,x_m)$ even though $p$ does not have all the variables $x_j$ in its expression. If $p$ does not depend on $x_j$ that will be seen in that $d_j=0$. A polynomial $p$ cannot, however, be expressed with the variable order $\vec{x}$ if not all its variables are in $\vec{x}$. A last note is that we can convert a polynomial $p$ with variable order $\vec{x}$ to any variable order $\vec{y}$ as long as $x_i\in\vec{y} \forall x_i\in\vec{x}$.

There are of course many operations that are needed when operating with polynomials; addition, subtraction, multiplication, division, modulo, gcd, checking for equality, finding roots and evaluating at a point. In the implementation all of them are defined recursively. We will show how this is done for all the most straight forward operations here, while the more complicated operations -- division, modulo and gcd -- are described later in section \ref{Ch: div,mod,gcd}.

First we define how we write that a polynomial is constructed:
\begin{definition}
  When we construct a polynomial $p$ of a variable $x$ then we write this as
  \begin{equation}
    p = polynomial(\vec{a},"x"),
  \end{equation}
  where $\vec{a}=(a_0,a_1,\ldots,a_d)$. This means that
  \begin{equation}
    p(x) = a_0 + a_1 x + \ldots + a_d x^d.
  \end{equation}
  Note that we have not specified what $a_i$ are; they can be either integers or polynomials -- but if they are polynomials they have to have the same variable order. Also note that once we have the polynomial $p$ we will use the notation $p[i]$ to reference the $i$-th variable, namely $a_i$. If we reference $p[j]$ where $j\ge d$, then this is defined as $p[j]=0 \forall j\ge d$.
\end{definition}

We also define the degree of a polynomial:
\begin{definition}
  Let the polynomial $p$ have variable order $\vec{x}=(x_1,\ldots,x_m)$. Then we define the degree of $p$, denoted $deg(p)$, as the largest exponent of $x_1$ in the expression for $p$.
\end{definition}

For all the operations we will use two polynomials $f$ and $g$. These are polynomials of at least the variable $k$, but can also be of other variables as well. We will now provide procedures that show how all the previously mentioned operations should work. Note that the procedures are assuming that the polynomials are on the same form, meaning that they assume the polynomials are of the same variables in the same order.

\begin{algorithm}
  \caption{Addition}
  \inp{Polynomials $f$ and $g$, with variable order $\vec{x}$}
  \outp{Polynomial $h$ such that $h=f+g$}
  \begin{algorithmic}[1]
    \Procedure{Add}{$f,g$}
      \If{$|\vec{x}|=1$}
        \State $a\gets [ ]$
        \For{$i\gets 0,\ldots,max(deg(f),deg(g))$}
          \State $a[i]\gets f[i]+g[i]$
        \EndFor
      \Else

      \EndIf
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\subsection{Division, modulo and GCD}\label{Ch: div,mod,gcd}


\section{Wilf-Zeilberger's method}
Wilf-Zeilberger's method is a method to show an equality by using a certifying pair, or a proof certificate. The methodology in the proof  is that assume we want to prove %~\cite{gf}
\begin{equation}\label{Eq: Theory,PS}
  \sum_{k=-\infty}^\infty A(n,k) = B(n).
\end{equation}
Then we let
\begin{equation}
  F(n,k)=\frac{A(n,k)}{B(n)}
\end{equation}
Now equation \ref{Eq: Theory,PS} becomes
\begin{equation}\label{Eq: Theory,WZ}
  \sum_{k=-\infty}^\infty F(n,k) = 1
\end{equation}
The way we want to prove this is by first proving
\begin{equation}\label{Eq: Theory,WZ2}
  \sum_{k=-\infty}^\infty F(n+1,k)-F(n,k) = 0
\end{equation}
and then calculate the left hand side of \ref{Eq: Theory,WZ} for one $n$, usually but not always $n=0$. Then we get that
\begin{equation}
  \sum_{k=-\infty}^\infty F(n+1,k)=\sum_{k=-\infty}^\infty F(n,k)
\end{equation}
which in turns gives us that
\begin{equation}
  \sum_{k=-\infty}^\infty F(n,k) = c,
\end{equation}
where $c$ is a constant, for all values of $n$ and thus equal to the result of our previous calculations. The trick we use to prove equation \ref{Eq: Theory,WZ2} is to find another function $G(n,k)$ which satisfies:
\begin{enumerate}[i)]
  \item
  \begin{equation}\label{Eq: Theory,first condition}
    F(n+1,k)-F(n,k)=G(n,k+1)-G(n,k)
  \end{equation}
  \item
  \begin{equation}\label{Eq: Theory,second condition}
    \lim_{k\to\pm\infty}G(n,k)=0, \forall n
  \end{equation}
\end{enumerate}
If these two conditions are fulfilled, then equation \ref{Eq: Theory,WZ2} is fulfilled since by replacing $F(n+1,k)-F(n,k)$ with $G(n,k+1)-G(n,k)$ we get a telescopic sum:
\begin{equation*}
\begin{split}
\sum_{k=-\infty}^\infty F(n+1,k)-F(n,k) & = \sum_{k=-\infty}^\infty G(n,k+1)-G(n,k) = \\
 & = \lim_{M\to\infty} \sum_{k=-M}^M G(n,k+1)-G(n,k) = \\
 & = \lim_{M\to\infty} \big[G(n,M+1)-G(n,-M)\big] = 0.
\end{split}
\end{equation*}
If we summarize the method we have the following steps:
\begin{enumerate}
  \item Start with equation on the form $$\sum_{k=-\infty}^\infty A(n,k) = B(n).$$
  \item Let $$F(n,k)=\frac{A(n,k)}{B(n)}.$$
  \item Find $G(n,k)$ such that equations \ref{Eq: Theory,first condition} and \ref{Eq: Theory,second condition} are fulfilled.
  \item Show that $\sum_{k=-\infty}^\infty F(0,k)=1$.
\end{enumerate}
We can formalize the concepts in the method like this.
\begin{definition}
  A pair of functions $(F,G)$ that satisfy equations \ref{Eq: Theory,first condition} and \ref{Eq: Theory,second condition} are said to ''certify'' an identity like \ref{Eq: Theory,WZ} (or is simply called a ''certifying pair''). We can also speak of a ''proof certificate'' $R(n,k)$ of an identity. That is a function such that $G(n,k)=R(n,k)F(n,k-1)$ gives that $(F,G)$ is a certifying pair to the identity.
\end{definition}
Let us show an short example of how the method can work.
\begin{example}
  Show that $$\sum_{k=0}^n \binom{n}{k} = 2^n.$$
\end{example}
\begin{solution}
  We will use the method and do all the steps mentioned above.
  \begin{enumerate}
    \item We have $A(n,k)=\binom{n}{k}$ and $B(n)=2^n$.
    \item Let $$F(n,k)=\frac{A(n,k)}{B(n)}=\frac{\binom{n}{k}}{2^n}.$$
    \item With $$G(n,k)=-\frac{\binom{n}{k-1}}{2^{n+1}},$$ we have that
      \begin{equation*}
        \begin{split}
          F(n+1,k)-F(n,k) & = \frac{\binom{n+1}{k}}{2^{n+1}}-\frac{\binom{n}{k}}{2^{n}}= \\
          & = \frac{1}{2^{n+1}}\binom{n}{k}\Bigg(\frac{n+1}{n+1-k}-2\Bigg) = \\
          & = \frac{1}{2^{n+1}}\binom{n}{k}\Bigg(\frac{k}{n+1-k}-1\Bigg) = \\
          & = \frac{1}{2^{n+1}}\Bigg(\binom{n}{k-1}-\binom{n}{k}\Bigg) = \\
          & = G(n,k+1)-G(n,k)
        \end{split}
      \end{equation*}
      Furthermore, we have that $G(n,k) = 0$ when $k<0$ and $k>n$.
    \item Lastly we need to show that $$\sum_{k=0}^n \frac{\binom{n}{k}}{2^n}=1$$ for some $n$. If we choose $n=0$ we see that this equality holds. Therefore the equality holds for all $n$.
  \end{enumerate}
\end{solution}
Now that we have seen an example, hopefully the methodology is clear. What still is not clear is how to find the function $G(n,k)$. This is in general a hard task to do by hand, but is certainly managable by using a computer since there are algorithms -- for instance Gosper's algorithm -- that can derive $G(n,k)$ in certain cases. This implementation is what this thesis is contributing to do. %~\cite{gf}

\section{Gosper's algorithm}
Gosper's algorithm is an algorithm that can be used to find an sum when certain conditions are fulfilled for the sum. The setting of the problem that is solved using the algorithm is that we want to find $S_k$ where
\begin{equation}\label{Eq: Theory,Gosper1}
  \sum_{k=1}^n a_k = S_n-S_0.
\end{equation}
This is the same thing as finding $S_n$ such that
\begin{equation}\label{Eq: Theory,Gosper2}
  a_k = S_k - S_{k-1}.
\end{equation}
The algorithm provides those $S_k$ such that
\begin{equation}
  \frac{S_k}{S_{k-1}} = \text{rational function of } k.
\end{equation}
Note that all the time when we write $x_a$ in this section, this denotes a polynomial $x$ evaluated in $a$, or $x(a)$. For instance $S_k$ denotes that the polynomial $S(k)$. We are only using this notation to make it easier to read. Note that this means that $x$ is a function of the variable $a$, but does not mean that it is a one variable function of $a$ -- it can be a function of several variables but we are just interested in the variable $a$ while the others are viewed as constants. This will actually be the cast amost all the time, since $a_k=F(n+1,k)-F(n,k)$ (where $F$ comes from Wilf-Zeilberger's method) will be used most of the time. Also here, note that $F(n,k)$ only denotes that $F$ is a function of $n$ and $k$, but it might be of more variables as well.

For the sake of making the thesis more easily readable and for completeness, we will provide a quite thorough derivation of the algorithm -- even though it is perfectly described in the original paper by Gosper (1978). This will mostly focus on the steps of the algorithm, but also include the proofs of claims that are made during the steps. Firstly we will describe the main steps and thereafter we will provide the proofs.

First of all we need to show the connection between the formulation of Wilf-Zeilberger's method and how Gosper's algorithm takes part in it. Gosper's algorithm will solve the third step in Wilf-Zeilbergers method, namely to find a $G(n,k)$ such that conditions \ref{Eq: Theory,first condition} and \ref{Eq: Theory,second condition} will be fulfilled. The first condition is that
\begin{equation}
  F(n+1,k)-F(n,k) = G(n,k+1)-G(n,k).
\end{equation}
By looking at $n$ as a constant we define $a_k=F(n+1,k)-F(n,k)$. Thereafter we use Gosper's algorithm to find a $S_k$ such that
\begin{equation}
  a_k = S_k-S_{k-1}.
\end{equation}
Once we have that, we let $G(n,k) = S_{k-1}$ and we have obtained our function $G(n,k)$.

The steps of Gosper's algorithm are:
\begin{enumerate}
  \item Finding polynomials $p_k,q_k,r_k$ such that
  \begin{equation}\label{Eq: Theory,Gosper,step1}
    \frac{a_k}{a_{k-1}} = \frac{p_k}{p_{k-1}}\frac{q_k}{r_k}
  \end{equation}
  and $\gcd(q_k,r_{k+j})=1, \forall j\geq 0$.
  \item Finding polynomial $f_k$ such that
  \begin{equation}\label{Eq: Theory,Gosper,step2}
    p_k=q_{k+1}f_k-r_kf_{k-1}.
  \end{equation}
  \item Let
  \begin{equation}\label{Eq: Theory,Gosper,S}
    S_k=\frac{q_{k+1}}{p_k}f_ka_k.
  \end{equation}
\end{enumerate}
The reason this algorithm will provide a solution is that with $S_k$ given by \ref{Eq: Theory,Gosper,S} we have
\begin{equation}\label{Eq: Theory,Gosper,sksk1}
  S_k-S_{k-1} = \frac{q_{k+1}}{p_k}f_ka_k-\frac{q_{k}}{p_{k-1}}f_{k-1}a_{k-1}.
\end{equation}
By factoring out $\frac{a_k}{p_k}$ in \ref{Eq: Theory,Gosper,sksk1}, and using \ref{Eq: Theory,Gosper,step1} followed by \ref{Eq: Theory,Gosper,step2} gives us:
\begin{equation}
  \begin{split}
    S_k-S_{k-1} & = \frac{a_k}{p_k}\Big(q_{k+1}f_k-\frac{q_k}{p_{k-1}}f_{k-1}p_k\frac{a_{k-1}}{a_k}\Big) = \\
    & =\frac{a_k}{p_k}\Big(q_{k+1}f_k-\frac{q_k}{p_{k-1}}f_{k-1}p_k\frac{p_{k-1}}{p_k}\frac{r_k}{q_k}\Big)= \\
    & = \frac{a_k}{p_k}\Big(q_{k+1}f_k-r_kf_{k-1}\Big) = \frac{a_k}{p_k}p_k = a_k,
  \end{split}
\end{equation}
which is exactly what we wanted to be true for $S_k$.

Now, we have quite a few details to derive in each step -- both how the step is done more precisely and furthermore explaining why all the polynomials actually need to be polynomials and not rational polynomials instead.

\subsection{How to get polynomials $p,q,r$ in equation \ref{Eq: Theory,Gosper,step1}}
First of all, we need to prove that it is possible to find polynomials $p,q,r$ such that \ref{Eq: Theory,Gosper,step1} is fulfilled. Since $\frac{S_k}{S_{k-1}}$ is a rational function of $k$, then
\begin{equation}
  \frac{a_k}{a_{k-1}}=\frac{S_k-S_{k-1}}{S_{k-1}-S_{k-2}}=\frac{\frac{S_k}{S_{k-1}}-1}{1-\frac{S_{k-2}}{S_{k-1}}}
\end{equation}
is a rational function of $k$ as well. Therefore we will be able to find polynomials such that \ref{Eq: Theory,Gosper,step1} is fulfilled. Left is to show how to find polynomials such that $gcd(q_k,r_{k+j})=1 \forall j\geq 1$ as well.

We do this in a series of steps. First we let $p_k=1$ and $q_k, r_k$ be the numerator and denominator of $\frac{a_k}{a_k-1}$, respectively. Then if $gcd(q_k,r_{k+j})=1, \forall j\geq 0$ we are done. Otherwise we replace the polynomials by
\begin{equation}
  \begin{split}
    q_k^\prime & \leftarrow \frac{q_k}{g_k}, \\
    r_k^\prime & \leftarrow \frac{r_k}{g_{k-j}}, \\
    p_k^\prime & \leftarrow p_kg_kg_{k-1}\ldots g_{k-j+1},
  \end{split}
\end{equation}
where $g_k=gcd(q_k,r_{k+j})$ for the smallest $j\geq 0$ such that $gcd(q_k,r_{k+j})\neq 1$. Then we can see that we still have that
\begin{equation}
  \frac{a_k}{a_{k-1}} = \frac{p_k^\prime}{p_{k-1}^\prime}\frac{q_k^\prime}{r_k^\prime}
\end{equation}
and the degree of the polynomials $q_k, r_k$ are smaller. We then iterate this procedure as long as there exists a $j$ such that $gcd(q_k,r_{k+j})>1$. Then when this procedure finishes, we will obtain $q_k,r_k,p_k$ such that \ref{Eq: Theory,Gosper,step1} is fulfilled and $gcd(q_k,r_{k+j})=1 \forall j\geq 0$.

\subsection{How to get polynomial $f$ in equation \ref{Eq: Theory,Gosper,step2}}
The proof provided here is just due to Gosper's paper and mostly very similar. First of all, we need to prove that $f$ is a polynomial and not a rational polynomial.
\begin{proof}
  Assume that $f_k=\frac{c_k}{d_k}$ where $gcd(c_k,d_k)=1$. Then by plugging this into \ref{Eq: Theory,Gosper,step2} and multiplying by $d_kd_{k-1}$ gives us:
  \begin{equation}\label{Eq: Theory,8prime}
    d_kd_{k-1}p_k = c_kd_{k-1}q_{k+1} - d_kc_{k-1}r_k.
  \end{equation}
  Let $j$ be the largest integer such that
  \begin{equation}\label{Eq: Theory,10a}
    gcd(d_k,d_{k+j})=g_k\neq 1
  \end{equation}
  Since $g_k|d_{k+j}$ we get that
  \begin{equation}\label{Eq: Theory,10b}
    gcd(d_{k-1},d_{k+j})=1=gcd(d_{k-1},g_{k+j}).
  \end{equation}
  By just shifting $k$ by $-j-1$ in \ref{Eq: Theory,10a} we get that
  \begin{equation}\label{Eq: Theory,10c}
    gcd(d_{k-j-1},d_{k-1})=g_{k-j-1}\neq 1
  \end{equation}
  By shifting $k$ by $j$ in \ref{Eq: Theory,10b} we get that
  \begin{equation}\label{Eq: Theory,10d}
    gcd(d_{k-j-1},d_k) = 1 = gcd(g_{k-j-1},d_k),
  \end{equation}
  again since $g_{k-j-1}|d_{k-j-1}$.

  Now we consider equation \ref{Eq: Theory,8prime} upon dividing by first $g_k$ and then $g_{k-j-1}$. Clearly $g_k|d_kd_{k-1}p_k$, since $g_k|d_k$. This means that
  \begin{equation}
    g_k|c_kd_{k-1}q_{k+1}-d_kc_{k-1}r_k.
  \end{equation}
  Furthermore $g_k|d_kc_{k-1}r_k$ since $g_k|d_k$, which gives us that
  \begin{equation}
    g_k|c_kd_{k-1}q_{k+1}.
  \end{equation}
  By equation \ref{Eq: Theory,10b} we get $gcd(d_{k-1},g_k)=1$ and since $g_k|d_k$ and $gcd(c_k,d_k)=1$ we get $gcd(c_k,g_k)=1$. This means that
  \begin{equation}
    g_k|q_{k+1} \implies g_{k-1}|q_k.
  \end{equation}

  Similarly $g_{k-j-1}|d_kd_{k-1}p_k$, since $g_{k-j-1}|d_{k-1}$. This means that
  \begin{equation}
    g_{k-j-1}|c_kd_{k-1}q_{k+1}-d_kc_{k-1}r_k.
  \end{equation}
  Furthermore $g_{k-j-1}|c_kd_{k-1}q_{k+1}$ since $g_{k-j-1}|d_{k-1}$, which gives us that
  \begin{equation}
    g_k|d_kc_{k-1}r_k.
  \end{equation}
  By equation \ref{Eq: Theory,10d} we get $gcd(d_k,g_{k-j-1})=1$ and by $gcd(c_{k-1},d_{k-1})=1$ together with $g_{k-j-1}|d_{k-1}$ we get $gcd(c_{k-1},g_{k-j-1})=1$. This means that
  \begin{equation}
    g_{k-j-1}|r_k \implies g_{k-1}|r_{k+j}.
  \end{equation}
  But now we have $g_{k-1}$ divides both $r_{k+j}$ where $j\geq 0$ and $q_k$. From the first step of Gosper's algorithm we know that $gcd(r_{k+j},q_k)=1, \forall j\geq 0$ meaning that $g_k=1$. This means that for all $j\geq 0$ we have that
  \begin{equation}
    gcd(d_k,d_{k+j})=1.
  \end{equation}
  By putting $j=0$ we get that
  \begin{equation}
    d_k = gcd(d_k,d_k) = 1.
  \end{equation}
  Hence $d_k=1$ for all $k$ and thus $f_k$ is a polynomial.
\end{proof}

Now, let us derive how to get the polynomial $f_k$ such that \ref{Eq: Theory,Gosper,step2} is fulfilled. It is possible to get a bound for the degree of $f_k$ with respect to $k$. This is derived in the paper by Gosper (1978) and for the interested reader the details can be found there. The results is that
\begin{equation}
  deg(f_k)\leq deg(p_k) - max(deg(q_{k+1}+r_k),deg(q_{k+1}-r_k)) + 1.
\end{equation}
In some cases the degree can be further limited (without the extra $+1$) but we are mostly interested in that we have a bound -- not the extra added (small) constant.

We will now firstly show that there is a way to get the polynomial $f_k$ given that the degree of $f_k$ is less than $N$ for all variables in $f_k$. This is done by assigning
\begin{equation}\label{Eq: Theory,general polynomial}
  f_k = \sum_{\substack{\vec{i}\in (0,1,\ldots,N)^{m-1}\\i_1\in (0,1,\ldots,N)}} \Big(a_{\vec{i},i_1}k^{i_1}\prod_{j=2}^m v_j^{i_j}\Big),
\end{equation}
where $m$ is the number of variables in $f_k$, the variables of $f_k$ are named $v_j$ for $1\leq j\leq m$ where $v_1=k$, $a_\vec{i}$ are the coefficients and $\vec{i}=(i_2,i_3,\ldots,i_m)$ goes through all combinations of degrees in the range $0\leq deg(v_j)=i_j\leq N$ for all the different variables. The number $i_1$ is the exponent of $k$ and also goes through all the values for in the range $0\leq i_1\leq N$. This exponent is treaded slightly different just because it is the exponent of $k$.

Then from this we can derive what the polynomial $f_{k-1}$ is. First we write the polynomial $f_{k-1}$ on the same form as in \ref{Eq: Theory,general polynomial} but with other coefficients:
\begin{equation}\label{Eq: Theory,general polynomial k-1}
  f_{k-1} = \sum_{\vec{i}\in (0,1,\ldots,N)^m} \Big(b_{\vec{i},i_1}k^{i_1}\prod_{j=2}^m v_j^{i_j}\Big).
\end{equation}
Now we will derive the expression for $b_{\vec{i},i_1}$ in terms of $a_{\vec{i},i_1}$. If we replace $k$ by $k-1$ in \ref{Eq: Theory,general polynomial} we get
\begin{equation}\label{Eq: Theory,general polynomial k-1 plugged in}
  \begin{split}
    f_k & = \sum_{\substack{\vec{i}\in (0,1,\ldots,N)^{m-1}\\i_1\in (0,1,\ldots,N)}} \Big(a_{\vec{i},i_1}(k-1)^{i_1}\prod_{j=2}^m v_j^{i_j}\Big) = \\
    & = \sum_{\substack{\vec{i}\in (0,1,\ldots,N)^{m-1}\\i_1\in (0,1,\ldots,N)}} \Big(a_{\vec{i},i_1}\Bigg(\sum_{u=0}^{i_1} \binom{i_1}{u}k^u (-1)^{i_1-u}\Bigg)\prod_{j=2}^m v_j^{i_j}\Big) = \\
    & = \sum_{\substack{\vec{i}\in (0,1,\ldots,N)^{m-1}\\i_1\in (0,1,\ldots,N)}} \Bigg(\sum_{u=0}^{i_1} a_{\vec{i},i_1}\binom{i_1}{u}(-1)^{i_1-u}k^u\Bigg) \prod_{j=2}^m v_j^{i_j}
  \end{split}
\end{equation}
Now we can see that the term $k^{i_1^\prime}\prod_{j=2}^m v_j^{i_j}$ from \ref{Eq: Theory,general polynomial k-1} ($i_1^\prime$ denotes a specific $i_1$) appears in \ref{Eq: Theory,general polynomial k-1 plugged in} in several places -- namely when $i_1\in\{i_1^\prime,i_1^\prime+1,\ldots,N\}$. This means that we get
\begin{equation}\label{Eq: Theory,fk-1 coefficients}
  b_{\vec{i},i_1} = \sum_{s=i_1}^N a_{\vec{i},s}\binom{s}{i_1}(-1)^{s-i_1}
\end{equation}
Now we have all the background we need in order to set up the system of equations and then solve for $a_{\vec{i},i_1}$. This is done by noticing that the left and right hand sides of the equation
\begin{equation}\label{Eq: Theory,f equation}
  p_k=q_{k+1}f_k - r_k f_{k-1}
\end{equation}
In order to ease the formulation of the equation system, let $\vec{\hat{i}}=(\vec{i},i_1)$, let $\ll$ denote that a vector is pointwise smaller than or equal to e.g. $(1,2,3)\ll(2,3,3)$ and let $\vec{i}-\vec{j}$ denote the pointwise difference between $\vec{i}$ and $\vec{j}$, e.g. $(2,3,3)-(1,2,3)=(1,1,0)$. Finally we also denote the coefficients in front of $k^{i_1}\prod_{j=2}^N v_j^{i_j}$ in the polynomials $p_k, q_{k+1}, r_k$ by $r_\vec{\hat{i}}, q_\vec{\hat{i}}, r_\vec{\hat{i}}$, respectively. This gives us that
\begin{equation}\label{Eq: Theory,equation system1}
  p_\vec{\hat{i}} = \sum_{\substack{\vec{\hat{j}}\\\vec{\hat{j}}\ll\vec{\hat{i}}\\\vec{\hat{j^\prime}}=\vec{\hat{i}}-\vec{\hat{j}}}} a_{\vec{\hat{j}}}q_{\vec{\hat{j^\prime}}} - \sum_{\substack{\vec{\hat{j}}\\\vec{\hat{j}}\ll\vec{\hat{i}}\\\vec{\hat{j^\prime}}=\vec{\hat{i}}-\vec{\hat{j}}}} b_{\vec{\hat{j}}}r_{\vec{\hat{j^\prime}}},
\end{equation}
where $b_{\vec{\hat{j}}}$ is given by \ref{Eq: Theory,fk-1 coefficients}. Note that at this point in the algorithm all the numbers $p_\vec{\hat{i}},q_\vec{\hat{i}},r_\vec{\hat{i}}$ are simply constants. This means that we can formulate a system of equations
\begin{equation}\label{Eq: Theory,equation system2}
  Ma=P,
\end{equation}
where $M$ is the matrix that is given by the right hand side of \ref{Eq: Theory,equation system1}, $a$ consists of all coefficients $a_{\vec{\hat{j}}}$ and $P$ is the left hand side of \ref{Eq: Theory,equation system1}. Note that each row in this system of equations correspond to a different $\vec{\hat{i}}$. Here we go through all $\vec{\hat{i}}$ that are relevant -- meaning all $\vec{\hat{i}}$ with degree smaller than what either of the sides of equation \ref{Eq: Theory,equation system1} can have. This means that we have an overdetermined system of equations. Furthermore we demand that all the coefficients of $f_k$ have to be integers. This means that it is not certain that we will get a solution, but in case we do we have found $f_k$. If we do not get a solution that can be because of two different reasons:
\begin{enumerate}[i)]
  \item We chose a too small maximal degree $N$ for the polynomial $f_k$.
  \item There does not exist a solution, hence the identity cannot be shown by Wilf-Zeilbergers method.
\end{enumerate}
In the paper by Gosper (1978) the first reason for not finding a $f_k$ does not exists, since the paper proves an upper limit on the degree of $f_k$ with respect to $k$. This cannot be done for other variables, for instance with $p_k=q_{k+1}=r_k=1$ any polynomial $f_k$ on the form $f_k=n^c+k$ satisfies \ref{Eq: Theory,f equation}. \todo[inline]{Is it possible to show if there exists a solution there exist one with limited degree}

In the system \ref{Eq: Theory,equation system2}, $M$ can be constructed to have dimension $L\times (N^m)$, where $L=max(deg(p_k)+1,max(deg(q_{k+1}),deg(r_k))+N+1)$. Here $deg(g)$ denotes the largest degree in any variable of the polynomial $g$. The reason for this limit is that the row with highest degree for any variable can be chosen to be $L$, since the degree of either side of the equation \ref{Eq: Theory,equation system1} is at most $L$.

The time complexity for this system of equations is $\mathcal{O}(L\cdot N^{2m})$, since we perform the following algorithm:
\begin{algorithm}
  \caption{Gaussian Elimination}
  \inp{Matrix $M$, vector $P$}
  \outp{Vector $x$ such that $Mx=P$}
  \begin{algorithmic}[1]
    \Procedure{GaussianElimination}{$M,P$}
      \State $r\gets \text{Number of rows in M}$
      \State $c\gets \text{Number of columns in M}$
      \For{$row\gets1,\ldots,c$}
        \State $i\gets $ first row with row number $\geq row $ and $M[i][c]\neq 0$
        \If{No such $i$ exists}
          \State Continue loop
        \EndIf
        \State $swap(M[row],M[i])$ \label{Extra linear term, start}
        \State $swap(P[row],P[i])$
        \For{$row_2\gets row+1,\ldots,L$}
          \State $g\gets gcd(M[row][c],M[row_2][c])$
          \State $M[row_2] \gets M[row_2]\cdot M[row][c]/g - M[row]\cdot M[row_2][c]/g$
          \State $P[row_2] \gets P[row_2]\cdot M[row][c]/g - P[row]\cdot M[row_2][c]/g$ \label{Extra linear term, end}
        \EndFor
      \EndFor
    \EndProcedure
  \end{algorithmic}
\end{algorithm}
Here we can clearly see that we have two loops, which are of size $c=N^m$ and $r=L$, respectively. The reason for the last $N^m$-factor is that when we perform row \ref{Extra linear term, start}--\ref{Extra linear term, end} we do this elementwise, hence once for each column.

This means that the time complexity is exponential in the number of variables. In general exponential time complexities are not desired at all -- since they grow very fast. In this thesis however we are working with identities that just involve a few (less than 5) different variables. Therefore this time complexity is acceptable. This means that given that given the limit on $m$ we have a polynomial time complexity for obtaining $f_k$.
