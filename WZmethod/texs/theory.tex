\section{Polynomials}
In the thesis we are continuously discussing polynomials. We are always using polynomials with integer coefficients -- which we also generalize to include rational coefficients by looking at fractions of polynomials with integer coefficients. Therefore, from now on whenever we talk about polynomials they are assumed to have integer coefficients as long as otherwise is not stated.

We will now have a discussion on what operations we want to be able to perform on the polynomials both theoreticly and partly implementation wise. Therefore there will not be a specific subchapter regarding polynomials later on in chapter \ref{Ch: Implementation}. But first we need to describe how a polynomial can be implemented and stored in the code -- and this is how it was implemented in this thesis.

A polynomial, say $p(n)$, can be seen as a list of coefficients $[a_0,a_1,\ldots,a_d]$. This represents the polynomial
\begin{equation}\label{Eq: Theory,polynomials,general polynomial}
  p(n) = a_0+a_1 n + \ldots + a_d n^d.
\end{equation}
This is fairly simple if the polynomial only has one variable. As soon as we introduce polynomials with more than one variable -- which is the case in all examples where Wilf-Zeilbergers method is used -- it gets a bit more complicated. The way we will visualize polynomials of more than one variable is to try to keep the very same structure as in \ref{Eq: Theory,polynomials,general polynomial}, namely we store a polynomial $p(n)$ as a list of coefficients $[a_0,a_1,\ldots,a_d]$, but instead of having $a_i$ just representing integers they are instead themselves polynomials. In order to show how this can be done, lets show an example.
\begin{example}\label{Ex: polynomial, recursive}
  Lets look at the polynomial
  \begin{equation*}
    \begin{split}
      p(k,m,n) = &1+3n+2n^2-m-2mn-2mn^2+3m^2+3m^2n+3m^2n^2+\\
      &3k-3kn+2kn^2+km-2kmn-km^2+2k^2-k^2n-k^2n^2+\\
      &3k^2m-3k^2mn-2k^2mn^2-3k^2m^2-k^2m^2n-3k^2m^2n^2.
    \end{split}
  \end{equation*}
  This will be stored as
  \begin{equation*}
    \begin{split}
      p(k,m,n) = & -((3k^2+3k-3)m^2-(2k+1)m-(3k^2-k+3))n^2+\\
      &((k^2-1)m^2-(k^2-2k+2)m-(2k+2))n+\\
      &((2k^2+2)m^2+(2k-1)m+(k^2-k-3))
    \end{split}
  \end{equation*}
  Note that this can be written as
  \begin{equation*}
    p(k,m,n) = p_2(k,m)n^2+p_1(k,m)n+p_0,
  \end{equation*}
  where
  \begin{equation*}
    \begin{split}
      p_0(k,m) &= ((2k^2+2)m^2+(2k-1)m+(k^2-k-3))\\
      p_1(k,m) &= ((k^2-1)m^2-(k^2-2k+2)m-(2k+2))\\
      p_2(k,m) &= -((3k^2+3k-3)m^2-(2k+1)m-(3k^2-k+3)).
    \end{split}
  \end{equation*}
  Now since our definition of polynomials is recursive, each of $p_0(k,m),p_1(k,m),p_2(k,m)$ is written as $q_2(k)m^2+q_1(k)m+q_0(k)$ for polynomials $q_0,q_1,q_2$. For instance for $p_1$ we have
  \begin{equation*}
    p_1(k,m) = q_2(k)m^2+q_1(k)m+q_0(k),
  \end{equation*}
  where
  \begin{equation*}
    \begin{split}
      q_0(k) & = -(2k+2) \\
      q_1(k) & = -(k^2-2k+2) \\
      q_2(k) & = (k^2-1)
    \end{split}
  \end{equation*}
  Lastly we have the polynomials $q_0,q_1,q_2$ which have integers as coefficients.
\end{example}
As we saw in example \ref{Ex: polynomial, recursive} we define all polynomials as in equation \ref{Eq: Theory,polynomials,general polynomial} where the coefficients $a_i$ are polynomials or integers, where it is only integers in the base case.

Note that in this case we wrote the polynomial with $n$ ''outmost'' in the representation of $p$. There was no specific reason for this, and the choice is not very important when only looking at one polynomial at a time. When we are using several polynomials at the same time, for instance if we want to add two polynomials, the structure is crucial. Therefore we will define some concepts regarding the representation of a polynomial.

\begin{definition}
  A polynomial $p$ is said to have the variable order $\vec{x}=(x_1,x_2,\ldots,x_m)$ if it is stored as
  \begin{equation}
    p(\vec{x}) = \sum_{i_1=0}^{d_1}\Bigg(\sum_{i_2=0}^{d_2}\Big(\sum_{i_3=0}^{d_3}\ldots\Big)x_2^{i_2}\Bigg)x_1^{i_1}.
  \end{equation}
  Note that the same polynomial can be expressed in various different ways, and we say that two variable orders, $\vec{x}$ and $\vec{y}$, are different if $|\vec{x}|\neq|\vec{y}|$ (where $|\vec{z}|$ denotes the length of the vector $\vec{z}$) or there exist an integer $i$ such that $x_i\neq y_i$.
\end{definition}
\begin{remark}
  If the polynomial $p$ has variable order $\vec{x}=(x_1,x_2,\ldots,x_m)$, then the coefficients of $p$ have variable order $\vec{x^\prime}=(x_2,\ldots,x_m)$.
\end{remark}
\begin{remark}
  Note that if the polynomial $p$ has variable order $\vec{x}$ such that $|\vec{x}|=1$, then the coefficients of $p$ are integers.
\end{remark}

Here we can note that $d_i$ is the degree of $p$ with respect to $x_i$. Furthermore we note that we can express a polynomial $p$ with the variable order $\vec{x}=(x_1,\ldots,x_m)$ even though $p$ does not have all the variables $x_j$ in its expression. If $p$ does not depend on $x_j$ that will be seen in that $d_j=0$. A polynomial $p$ cannot, however, be expressed with the variable order $\vec{x}$ if not all its variables are in $\vec{x}$. A last note is that we can convert a polynomial $p$ with variable order $\vec{x}$ to any variable order $\vec{y}$ as long as $x_i\in\vec{y} \forall x_i\in\vec{x}$.

There are of course many operations that are needed when operating with polynomials; addition, negate, subtraction, multiplication, division, modulo, gcd (greatest common divisor), checking for equality, finding roots and evaluating at a point. In the implementation all of them are defined recursively. We will show how this is done for all the most straight forward operations here, while the more complicated operations -- division, modulo and gcd -- are described later in section \ref{Ch: div,mod,gcd}.

First we define how we write that a polynomial is constructed:
\begin{definition}
  When we construct a polynomial $p$ of a variable $x$ then we write this as
  \begin{equation}
    p = polynomial(\vec{a},"x"),
  \end{equation}
  where $\vec{a}=(a_0,a_1,\ldots,a_d)$. This means that
  \begin{equation}
    p(x) = a_0 + a_1 x + \ldots + a_d x^d.
  \end{equation}
  Note that we have not specified what $a_i$ are; they can be either integers or polynomials -- but if they are polynomials they have to have the same variable order. Also note that once we have the polynomial $p$ we will use the notation $p[i]$ to reference the $i$-th variable, namely $a_i$. If we reference $p[j]$ where $j\ge d$, then this is defined as $p[j]=0 \forall j\ge d$.
\end{definition}

We also define the degree of a polynomial:
\begin{definition}
  Let the polynomial $p$ have variable order $\vec{x}=(x_1,\ldots,x_m)$. Then we define the degree of $p$, denoted $deg(p)$, as the largest exponent of $x_1$ in the expression for $p$.
\end{definition}

For all the operations we will use two polynomials $f$ and $g$. These are polynomials with the variable order $\vec{x}$. We will now provide procedures that show how all the previously mentioned operations should work. Note that the procedures are assuming that the polynomials have the same variable order, if this is not the case then the polynomials first are converted into a common variable order.

All the procedures try to follow the same structure, which is quite recursive. All procedures have a base case, usually when $|\vec{x}|=1$, and in all other cases recursive calls are made. A note to be made here is that recursive calls are not computationally heavy in theory but might take time if many are made, especially if the depth is large, in practice. In this thesis however, the depth is very small -- usually less than 5 -- since the depth comes from the number of variables in the polynomials. Therefore the recursion is not expected to cause any problems computationally.

\begin{algorithm}[H]
  \caption{Addition}
  \inp{Polynomials $f$ and $g$, with variable order $\vec{x}$}
  \outp{Polynomial $h$ such that $h=f+g$}
  \begin{algorithmic}[1]
    \Procedure{Add}{$f,g$}
      \State $\vec{a}\gets \emptyvec$
      \For{$i\gets 0,\ldots,max(deg(f),deg(g))$}
        \If{$|\vec{x}|=1$}
          \State $\vec{a}[i]\gets f[i]+g[i]$
        \Else
          \State $\vec{a}[i]\gets$ \Call{Add}{$f[i],g[i]$}
        \EndIf
      \EndFor
      \State \Return $polynomial(\vec{a},\vec{x}[0])$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
  \caption{Negatation}
  \inp{Polynomial $f$ with variable order $\vec{x}$}
  \outp{Polynomial $h$ such that $h=-f$}
  \begin{algorithmic}[1]
    \Procedure{Negate}{$f$}
      \State $\vec{a}\gets \emptyvec$
      \For{$i\gets 0,\ldots,deg(f)$}
        \If{$|\vec{x}|=1$}
          \State $\vec{a}[i]\gets -f[i]$
        \Else
          \State $\vec{a}[i]\gets$ \Call{Negate}{$f[i]$}
        \EndIf
      \EndFor
      \State \Return $polynomial(\vec{a},\vec{x}[0])$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
  \caption{Subtraction}
  \inp{Polynomials $f$ and $g$, with variable order $\vec{x}$}
  \outp{Polynomial $h$ such that $h=f-g$}
  \begin{algorithmic}[1]
    \Procedure{Subtract}{$f,g$}
      \State $g^\prime\gets$ \Call{Negate}{$g$}
      \State \Return \Call{Add}{$f,g^\prime$}
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
  \caption{Multiplication}
  \inp{Polynomials $f$ and $g$, with variable order $\vec{x}$}
  \outp{Polynomial $h$ such that $h=f\cdot g$}
  \begin{algorithmic}[1]
    \Procedure{Multiply}{$f,g$}
      \State $\vec{a}\gets \emptyvec$
      \For{$i\gets 0,\ldots,deg(f)+deg(g)$}
        \State $\vec{a}[i]\gets 0$
      \EndFor
      \For{$i\gets 0,\ldots,deg(f)$}
        \For{$j\gets 0,\ldots,deg(g)$}
          \If{$|\vec{x}|=1$}
            \State $\vec{a}[i+j]\gets \vec{a}[i+j] + f[i]\cdot g[j]$
          \Else
            \State $y_{i,j}\gets$ \Call{Multiply}{$f[i],g[j]$}
            \State $\vec{a}[i+j]\gets$ \Call{Add}{$\vec{a}[i+j],y_{i,j}$}
          \EndIf
        \EndFor
      \EndFor
      \State \Return $polynomial(\vec{a},\vec{x}[0])$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
  \caption{Equality}
  \inp{Polynomials $f$ and $g$, with variable order $\vec{x}$}
  \outp{$True$ if $f$ and $g$ are equal, otherwise $False$}
  \begin{algorithmic}[1]
    \Procedure{Equals}{$f,g$}
      \If{$deg(f)\neq deg(g)$}
        \State \Return $False$
      \EndIf
      \For{$i\gets 0,\ldots,deg(f)$}
        \If{$|\vec{x}|=1$ AND $f[i]\neq g[i]$}
          \State \Return $False$
        \ElsIf{$|\vec{x}|>1$ AND $not$ \Call{Equals}{$f[i],g[i]$}}
          \State \Return $False$
        \EndIf
      \EndFor
      \State \Return $True$
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
  \caption{Evaluating at a point}
  \inp{Polynomials $f$, with variable order $\vec{x}$, variable $v\in\vec{x}$ and value $y$}
  \outp{Polynomial $h$ that is $f$ evaluated at $v=y$}
  \begin{algorithmic}[1]
    \Procedure{Evaluate}{$f,v,y$}
      \If{$\vec{x}[0]=v$}
        \State $g\gets f[0]$
        \For{$i\gets 1,\ldots,deg(f)$}
          \State $h\gets$ \Call{Multiply}{$f[i],y^i$}
          \State $g\gets$ \Call{Add}{$g,h$}
        \EndFor
        \State \Return $g$
      \Else
        \State $\vec{a}\gets\emptyvec$
        \For{$i\gets 0,\ldots,deg(f)$}
          \State $\vec{a}[i]\gets$ \Call{Evaluate}{$f[i],v,y$}
        \EndFor
        \State \Return $polynomial(\vec{a},\vec{x}[0])$
      \EndIf
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

The next procedure is used to find roots of a polynomial. Here if the polynomial is only dependent on one variable, meaning $|\vec{x}|=1$, then we solve this equation (we call this function $SolveEquation$). In the implementation of the program only solving equations up to degree 2 is implemented, but the program also tries to insert values in the range $(-100,100)$ and sees if any of these are roots. We do not include the procedure for $SolveEquation$ in the report, since it is just solving a second degree equation and additionally trying a bunch of different small values.
\begin{algorithm}[H]
  \caption{Finding roots}
  \inp{Polynomial $f$, with variable order $\vec{x}=(x_1,\ldots,x_m)$}
  \outp{Integers $x^\prime$ such that $x_m=x^\prime \implies f=0$}
  \begin{algorithmic}[1]
    \Procedure{Roots}{$f$}
      \If{$|\vec{x}|=1$}
        \State \Return \Call{SolveEquation}{$f$}
      \Else
        \State $\vec{a}\gets \Call{Roots}{f[0]}$
        \For{$i\gets 1,\ldots,deg(f)$}
          \State $\vec{b}\gets \emptyvec$
          \ForAll{$a^\prime\in \vec{a}$}
            \If{\Call{Evaluate}{$f[i],x_m,a^\prime$}=0}
              \State $\vec{b}[size(\vec{b})]\gets a^\prime$
            \EndIf
          \EndFor
          \State $\vec{a}\gets \vec{b}$
        \EndFor
        \State \Return $\vec{a}$
      \EndIf
    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\subsection{Division, modulo and GCD}\label{Ch: div,mod,gcd}
Now we come to the slightly more complicated operations that we need to perform on polynomials. First we define GCD for polynomials a bit more carefully, which we do in a few steps.
\begin{definition}\label{Def: divide}
  A polynomial $g$ is said to divide another polynomial $a$ if there exists a polynomial $q$ such that $a=g\cdot q$. We will use the notation $g|a$ to denote that $g$ divides $a$.
\end{definition}
\begin{definition}\label{Def: common divisor}
  A polynomial $g$ is called a \textit{common divisor} to two polynomials $a$ and $b$ if $g|a$ and $g|b$.
\end{definition}
\begin{definition}\label{Def: greatest common divisor}
  We say that a polynomial $g$ is the \textit{greatest common divisor} (GCD) to two polynomials if
  \begin{enumerate}
    \item $g$ is a common divisor to $a$ and $b$
    \item For all common divisors $g^\prime$ to $a$ and $b$ we have that $g^\prime|g$.
  \end{enumerate}
\end{definition}
In general performing division, modulo and gcd is fairly straight forward even on polynomials. Usually one might have the following structure on the implementation:
\begin{equation}\label{Eq: Theory,standard divide}
  DIVIDE(a,b) \rightarrow (q,r) \text{ such that } a=b\cdot q + r, deg(r) < deg(b),
\end{equation}
\begin{equation}
  MODULO(a,b) \rightarrow r \text{ where } DIVIDE(a,b) = (q,r),
\end{equation}
\begin{equation}\label{Eq: Theory,standard gcd}
  GCD(a,b) \rightarrow a \text{ if } b=0 \text{ otherwise } GCD(b,MODULO(a,b)),
\end{equation}
where $\rightarrow$ indicates what the function returns.

This implementation scheme is fairly straight forward, even though the divide function might be a bit messy. The problem we encounter in this thesis though, is that we cannot necessarily find polynomials $q,r$ such that \ref{Eq: Theory,standard divide} is satisfied. The reason for this is that we are working with integer coefficients and not in a ring. In order to show the problems we encounter we will use a common example throughout this section to show what the problem is and how we solve it.
\begin{example}
  Consider dividing $a(x)=x^2+2x+1$ by $b(x)=2x$. Assume we have $q,r$ that fulfill \ref{Eq: Theory,standard divide}. Then we need to have $deg(r)<deg(b)=deg(2x)=1$. This means that $r(x)=c$, where $c$ is an integer constant. This gives us
  \begin{equation}\label{Eq: poly,divide,ex1}
    x^2+2x+1=(2x)q(x)+c.
  \end{equation}
  Furthermore we have that $deg(\text{Left hand side})=2$, hence $deg(q)=1$. Let $q(x)=c_0+c_1x$, then \ref{Eq: poly,divide,ex1} becomes
  \begin{equation}
    x^2+2x+1=(2x)(c_1x+c_0)+c=2c_1x^2+2c_0x+c.
  \end{equation}
  By looking at degree 2 we get $2c_1=1$, which of course does not have any solutions for integers $c_1$.
\end{example}

The way we solve this is to instead of giving two polynomials $q,r$ when we divide $a$ by $b$ we also give a third parameter, $f$, which is a factor of the same type as the coefficients in $a$ and $b$. The condition that needs to be fulfilled is
\begin{equation}\label{Eq: Theory,divide condition}
  DIVIDE(a,b) \rightarrow (q,r,f) \text{ such that } f\cdot a=b\cdot q + r, deg(r) < deg(b).
\end{equation}
Now we will show that it actually is possible to find $q,r,f$ such that \ref{Eq: Theory,divide condition} holds.
\begin{theorem}
  Let $a(x_1),b(x_1)$ be polynomials of the same variable order $\vec{x}=(x_1,\ldots,x_m)$. Then it is possible to find polynomials $q,r,f$, where $q,r$ have variable order $\vec{x}$ and $f$ has variable order $\vec{x^\prime}=(x_2,\ldots,x_m)$, such that
  \begin{equation}\label{Eq: Theory,theorem,divide}
    f\cdot a(x_1)=q(x_1)b(x_1) + r(x_1),
  \end{equation}
  and $deg(r)<deg(b)$.
\end{theorem}
\begin{proof}
  Let $a=a_0+a_1x_1+\ldots+a_cx_1^c$ and $b=b_0+b_1x_1+\ldots+b_dx_1^d$, where $c,d$ are the degrees of $a$ and $b$, respectively. If $c<d$ then we can use $q=0, r=a, f=1$ and we have that \ref{Eq: Theory,theorem,divide} is fulfilled.

  Now assume that $c\geq d$. We will now show that we can find $a^\prime,q_0,f_0$ such that
  \begin{equation}
    f_0a(x_1) = q_0(x_1)b(x_1) + a^\prime(x_1).
  \end{equation}
  This is obtained by choosing $f_0=b_d, q_0=a_cx^{c-d}$ and $a^\prime(x_1)=b_da(x_1)-a_cx^{c-d}b(x_1)$. This gives us
  \begin{equation}\label{Eq: Theory,aprime}
    \begin{split}
      a^\prime(x_1) & = b_da(x_1)-a_cx^{c-d}b(x_1) = b_d(a_0+\ldots+a_cx^c)-a_cx^{c-d}(b_0+\ldots+b_dx^d) \\
      & = b_d(a_0+\ldots+a_{c-1}x^{c-1})-a_cx^{c-d}(b_0+\ldots+b_{d-1}x^{d-1})
    \end{split}
  \end{equation}
  Now we can clearly see that $deg(a^\prime)<c$, which means we have reduced the problem by (at least) 1 in the degree of $a$. We continue to do this until the degree of $a$ is less than the degree of $b$, and then we collect the results. If we have that
  \begin{equation}
    DIVIDE(a^\prime,b) \rightarrow (q^\prime,r^\prime,f^\prime),
  \end{equation}
  then since $a^\prime$ is given by \ref{Eq: Theory,aprime} we get
  \begin{equation}
    \begin{split}
      f^\prime\cdot a^\prime & =q^\prime\cdot b + r^\prime \hspace{2cm}\implies \\
      f^\prime (b_da-a_cx^{c-d}b) & =q^\prime\cdot b + r^\prime \hspace{2cm} \implies \\
      (fb_d)a & = (q^\prime + a_cx^{c-d})b + r^\prime,
    \end{split}
  \end{equation}
  and thus we get
  \begin{equation}
    DIVIDE(a,b) \rightarrow (q,r,f) = (q^\prime + a_cx^{c-d},r^\prime,fb_d).
  \end{equation}
  Now we have shown the theorem, and provided the method for dividing polynomials at once.
\end{proof}
\begin{definition}
  When we divide polynomial $a$ by polynomial $b$ (both having variable order $\vec{x}=(x_1,\ldots,x_m)$) we are given three things:
  \begin{itemize}
    \item $q$, a polynomial with variable order $\vec{x}$, which is called the ''quotient'',
    \item $r$, a polynomial with variable order $\vec{x}$, which is called the ''remainder'',
    \item $f$, a polynomial with variable order $(x_2,\ldots,x_m)$, which is called the ''factor''.
  \end{itemize}
  These polynomials fulfill equation \ref{Eq: Theory,divide condition}.
\end{definition}
\begin{remark}
  One important thing to note is that the definition of how we divide is not unique, if $q,r,f$ are all multiplied by a polynomial $s$ with variable order $(x_2,\ldots,x_m)$ then equation \ref{Eq: Theory,divide condition} will still be fulfilled.
\end{remark}

Now we have defined division and will soon move over to calculating the greatest common divisor, but first we have a look at what happens to our example.
\begin{example}
  Consider dividing $a(x)=x^2+2x+1$ by $b(x)=2x$. Now we see that $q=x+2,r=2,f=2$ gives us a solution where $0=deg(r)<deg(b)=1$.
\end{example}

We will show a theorem which states how the greatest common divisor is computed, but first we need a little bit of notation.
\begin{definition}
  Let $p$ be a polynomial with variable order $\vec{x}=(x_1,\ldots,x_m)$. If we write $p$ as $p(x_1)=p_0+p_1x_1+\ldots p_dx_1^d$, then we denote the greatest common divisor of all the coefficients $p_i, 0\leq i\leq d$ by $gcd_c(p)$.
\end{definition}
\begin{theorem}\label{Thm: gcd}
  Let $g$ denote the greatest common divisor of polynomials $a$ and $b$. Then we can compute this by the following:
  \begin{enumerate}
    \item If $b=0$, then $g=a$,
    \item Otherwise $g$ is given by
    \begin{equation}\label{Eq: gcd}
      g=\frac{g^\prime}{gcd_c(g^\prime)}\cdot gcd(gcd_c(a),gcd_c(b)),
    \end{equation}
    where $g^\prime$ is the greatest common divisor of $b$ and $r$, where $r$ is the remainder when $a$ is divided by $b$, which means
    \begin{equation}
      DIVIDE(a,b) \rightarrow (q,r,f).
    \end{equation}
  \end{enumerate}
\end{theorem}
Before we prove theorem \ref{Thm: gcd} we show the reasoning behind why we get this more complicated version of gcd in \ref{Eq: gcd} instead of the much easier version in \ref{Eq: Theory,standard gcd}. We start by showing the example and see what happens if we naïvely using the new division combined with \ref{Eq: Theory,standard gcd}.
\begin{example}\label{Ex: gcd}
  Consider computing the greatest common divisor of $a(x)=x^2+2x+1$ and $b(x)=2x$ using \ref{Eq: Theory,standard gcd}.
  \begin{equation}
    gcd(x^2+2x+1,2x)=gcd(2x,2)=gcd(2,0)=2,
  \end{equation}
  since
  \begin{equation}
    \begin{split}
      DIVIDE(x^2+2x+1,2x) & \rightarrow (q,r,f)=(x+2,2,2) \\
      DIVIDE(2x,2) & \rightarrow (q,r,f)=(x,0,1).
    \end{split}
  \end{equation}
  But now we see that we have the greatest common divisor being 2, but 2 does not divide $a(x)$. This highlights a problem with using \ref{Eq: Theory,standard gcd} for gcd.

  Now let us consider using \ref{Eq: gcd} instead. This gives us (working backwards in steps)
  \begin{equation}
    \begin{split}
      gcd(2,0) &= 2 \\
      gcd(2x,2) = \frac{gcd(2,0)}{gcd_c(gcd(2,0))}\cdot gcd(gcd_c(2x),gcd_c(2))= \frac{2}{2}\cdot gcd(2,2) &= 2 \\
      gcd(x^2+2x+1,2x) =\frac{gcd(2x,2)}{gcd_c(gcd(2x,2))}\cdot gcd(gcd_c(x^2+2x+1),gcd_c(2x)) & = \\ = \frac{2}{2}\cdot gcd(1,2) & = 1.
    \end{split}
  \end{equation}
  Using \ref{Eq: gcd} seems to work on this example. We need, however to prove it in general and to prove it regardless of which $q,r,f$ are used for $DIVIDE(a,b)$.
\end{example}
As we saw in example \ref{Ex: gcd} if we try to naïvely use \ref{Eq: Theory,standard gcd} to compute the gcd, but with the new definition of how we divide, then that results in a too large divisor since the factor $f$ allows the divisor to be larger. This can be seen by looking at the formula for dividing $a$ by $b$. We have that
\begin{equation}\label{Eq: Theory, gcd, explanation}
  f\cdot a(x_1)=b(x_1)\cdot q(x_1) + r(x_1).
\end{equation}
Now the greatest common divisor of $b$ and $r$ will divide the right hand side $f\cdot a(x_1)$ but not necessarily $a(x_1)$. With this insight in mind, we will now prove theorem \ref{Thm: gcd}, but first we need a lemma.
\begin{lemma}
  Let $p$ be a polynomial $p(x_1)=p_0+\ldots+p_dx_1^d$ with variable order $\vec{x}=(x_1,\ldots,x_m)$ and $f$ be a polynomial with variable order $(x_2,\ldots,x_m)$ such that $f|p$. Then we have that $f|p_i\forall 0\leq i\leq d$.
\end{lemma}
\begin{proof}
  We know that $f|p$, which is equivalent to $p=fp^\prime$ for some polynomial $p^\prime$. Let $p^\prime=p_0^\prime+\ldots+p_c^\prime x_1^c$. Now first of all, since $f$ does not depend on $x_1$ we have that $deg(p)=deg(p^\prime)=d$. Furthermore we have that
  \begin{equation}\label{Eq: lemma proof}
    \begin{split}
      p_0+p_1x_1+\ldots+p_dx_1^d=p=fp &=f(p_0^\prime+p_1^\prime x_1+\ldots+p_d^\prime x_1^d)= \\
      &=(fp_0^\prime)+(fp_1^\prime)x_1+\ldots+(fp_d^\prime)x_1^d
    \end{split}
  \end{equation}
  Now by looking at the terms for each degree of $x_1$ in \ref{Eq: lemma proof} we see that
  \begin{equation}
    p_i=fp_i^\prime \iff f|p_i, \forall 0\leq i\leq d.
  \end{equation}
\end{proof}
\begin{proof}[Theorem \ref{Thm: gcd}]
  \todo[inline]{finish it}
  We done $gcd(b,r)$ by $g^\prime$. This gives us that, since $g^\prime|b$ and $g^\prime|r$, $g^\prime$ divides the left hand side of \ref{Eq: Theory, gcd, explanation}. But since the right hand side is equal to the left hand side, we know that $g^\prime|fa$. This in turns implies that we can factor $g$ into two parts, $g_f,g_a$, such that $g=g_ag_f$, $g_a|a$ and $g_f|f$. But now since $f$ has variable order $(x_2,\ldots,x_m)$ then $g_f$ cannot depend on $x_1$.
\end{proof}
Although the procedures of division and gcd are partly already described in the theorems and proofs in this section we will finish this section by formalize the procedures of division and gcd, by providing the pseudocode.
\todo[inline]{Write algos}
\begin{algorithm}[H]
  \caption{Division}
  \inp{Polynomial $a$ and $b$, with variable order $\vec{x}=(x_1,\ldots,x_m)$}
  \outp{Polynomials $q,r$ with variable order $\vec{x}=(x_1,\ldots,x_m)$ and polynomial $f$ with variable order $\vec{x}=(x_2,\ldots,x_m)$ such that $f\cdot a=q\cdot b + r$ and $deg(r)<deg(b)$}
  \begin{algorithmic}[1]
    \Procedure{Divide}{$a,b$}

    \EndProcedure
  \end{algorithmic}
\end{algorithm}
\begin{algorithm}[H]
  \caption{Greatest common divisor}
  \inp{Polynomial $a$ and $b$, with variable order $\vec{x}=(x_1,\ldots,x_m)$}
  \outp{Polynomial $g$ with variable order $\vec{x}=(x_1,\ldots,x_m)$ such that
  $g$ is the greatest common divisor of $a$ and $b$}
  \begin{algorithmic}[1]
    \Procedure{GCD}{$a,b$}

    \EndProcedure
  \end{algorithmic}
\end{algorithm}

\section{Wilf-Zeilberger's method}
Wilf-Zeilberger's method is a method to show an equality by using a certifying pair, or a proof certificate. The methodology in the proof is that assume we want to prove %~\cite{gf}
\begin{equation}\label{Eq: Theory,PS}
  \sum_{k=-\infty}^\infty A(n,k) = B(n).
\end{equation}
Then we let
\begin{equation}
  F(n,k)=\frac{A(n,k)}{B(n)}
\end{equation}
Now equation \ref{Eq: Theory,PS} becomes
\begin{equation}\label{Eq: Theory,WZ}
  \sum_{k=-\infty}^\infty F(n,k) = 1
\end{equation}
The way we want to prove this is by first proving
\begin{equation}\label{Eq: Theory,WZ2}
  \sum_{k=-\infty}^\infty F(n+1,k)-F(n,k) = 0
\end{equation}
and then calculate the left hand side of \ref{Eq: Theory,WZ} for one $n$, usually but not always $n=0$. Then we get that
\begin{equation}
  \sum_{k=-\infty}^\infty F(n+1,k)=\sum_{k=-\infty}^\infty F(n,k)
\end{equation}
which in turns gives us that
\begin{equation}
  \sum_{k=-\infty}^\infty F(n,k) = c,
\end{equation}
where $c$ is a constant, for all values of $n$ and thus equal to the result of our previous calculations. The trick we use to prove equation \ref{Eq: Theory,WZ2} is to find another function $G(n,k)$ which satisfies:
\begin{enumerate}[i)]
  \item
  \begin{equation}\label{Eq: Theory,first condition}
    F(n+1,k)-F(n,k)=G(n,k+1)-G(n,k)
  \end{equation}
  \item
  \begin{equation}\label{Eq: Theory,second condition}
    \lim_{k\to\pm\infty}G(n,k)=0, \forall n
  \end{equation}
\end{enumerate}
If these two conditions are fulfilled, then equation \ref{Eq: Theory,WZ2} is fulfilled since by replacing $F(n+1,k)-F(n,k)$ with $G(n,k+1)-G(n,k)$ we get a telescopic sum:
\begin{equation*}
\begin{split}
\sum_{k=-\infty}^\infty F(n+1,k)-F(n,k) & = \sum_{k=-\infty}^\infty G(n,k+1)-G(n,k) = \\
 & = \lim_{M\to\infty} \sum_{k=-M}^M G(n,k+1)-G(n,k) = \\
 & = \lim_{M\to\infty} \big[G(n,M+1)-G(n,-M)\big] = 0.
\end{split}
\end{equation*}
If we summarize the method we have the following steps:
\begin{enumerate}
  \item Start with equation on the form $$\sum_{k=-\infty}^\infty A(n,k) = B(n).$$
  \item Let $$F(n,k)=\frac{A(n,k)}{B(n)}.$$
  \item Find $G(n,k)$ such that equations \ref{Eq: Theory,first condition} and \ref{Eq: Theory,second condition} are fulfilled.
  \item Show that $\sum_{k=-\infty}^\infty F(0,k)=1$.
\end{enumerate}
We can formalize the concepts in the method like this.
\begin{definition}
  A pair of functions $(F,G)$ that satisfy equations \ref{Eq: Theory,first condition} and \ref{Eq: Theory,second condition} are said to ''certify'' an identity like \ref{Eq: Theory,WZ} (or is simply called a ''certifying pair''). We can also speak of a ''proof certificate'' $R(n,k)$ of an identity. That is a function such that $G(n,k)=R(n,k)F(n,k-1)$ gives that $(F,G)$ is a certifying pair to the identity.
\end{definition}
Let us show an short example of how the method can work.
\begin{example}
  Show that $$\sum_{k=0}^n \binom{n}{k} = 2^n.$$
\end{example}
\begin{solution}
  We will use the method and do all the steps mentioned above.
  \begin{enumerate}
    \item We have $A(n,k)=\binom{n}{k}$ and $B(n)=2^n$.
    \item Let $$F(n,k)=\frac{A(n,k)}{B(n)}=\frac{\binom{n}{k}}{2^n}.$$
    \item With $$G(n,k)=-\frac{\binom{n}{k-1}}{2^{n+1}},$$ we have that
      \begin{equation*}
        \begin{split}
          F(n+1,k)-F(n,k) & = \frac{\binom{n+1}{k}}{2^{n+1}}-\frac{\binom{n}{k}}{2^{n}}= \\
          & = \frac{1}{2^{n+1}}\binom{n}{k}\Bigg(\frac{n+1}{n+1-k}-2\Bigg) = \\
          & = \frac{1}{2^{n+1}}\binom{n}{k}\Bigg(\frac{k}{n+1-k}-1\Bigg) = \\
          & = \frac{1}{2^{n+1}}\Bigg(\binom{n}{k-1}-\binom{n}{k}\Bigg) = \\
          & = G(n,k+1)-G(n,k)
        \end{split}
      \end{equation*}
      Furthermore, we have that $G(n,k) = 0$ when $k<0$ and $k>n$.
    \item Lastly we need to show that $$\sum_{k=0}^n \frac{\binom{n}{k}}{2^n}=1$$ for some $n$. If we choose $n=0$ we see that this equality holds. Therefore the equality holds for all $n$.
  \end{enumerate}
\end{solution}
Now that we have seen an example, hopefully the methodology is clear. What still is not clear is how to find the function $G(n,k)$. This is in general a hard task to do by hand, but is certainly managable by using a computer since there are algorithms -- for instance Gosper's algorithm -- that can derive $G(n,k)$ in certain cases. This implementation is what this thesis is contributing to do. %~\cite{gf}

\section{Gosper's algorithm}
Gosper's algorithm is an algorithm that can be used to find an sum when certain conditions are fulfilled for the sum. The setting of the problem that is solved using the algorithm is that we want to find $S_k$ where
\begin{equation}\label{Eq: Theory,Gosper1}
  \sum_{k=1}^n a_k = S_n-S_0.
\end{equation}
This is the same thing as finding $S_n$ such that
\begin{equation}\label{Eq: Theory,Gosper2}
  a_k = S_k - S_{k-1}.
\end{equation}
The algorithm provides those $S_k$ such that
\begin{equation}
  \frac{S_k}{S_{k-1}} = \text{rational function of } k.
\end{equation}
Note that all the time when we write $x_a$ in this section, this denotes a polynomial $x$ evaluated in $a$, or $x(a)$. For instance $S_k$ denotes that the polynomial $S(k)$. We are only using this notation to make it easier to read. Note that this means that $x$ is a function of the variable $a$, but does not mean that it is a one variable function of $a$ -- it can be a function of several variables but we are just interested in the variable $a$ while the others are viewed as constants. This will actually be the cast amost all the time, since $a_k=F(n+1,k)-F(n,k)$ (where $F$ comes from Wilf-Zeilberger's method) will be used most of the time. Also here, note that $F(n,k)$ only denotes that $F$ is a function of $n$ and $k$, but it might be of more variables as well.

For the sake of making the thesis more easily readable and for completeness, we will provide a quite thorough derivation of the algorithm -- even though it is perfectly described in the original paper by Gosper (1978). This will mostly focus on the steps of the algorithm, but also include the proofs of claims that are made during the steps. Firstly we will describe the main steps and thereafter we will provide the proofs.

First of all we need to show the connection between the formulation of Wilf-Zeilberger's method and how Gosper's algorithm takes part in it. Gosper's algorithm will solve the third step in Wilf-Zeilbergers method, namely to find a $G(n,k)$ such that conditions \ref{Eq: Theory,first condition} and \ref{Eq: Theory,second condition} will be fulfilled. The first condition is that
\begin{equation}
  F(n+1,k)-F(n,k) = G(n,k+1)-G(n,k).
\end{equation}
By looking at $n$ as a constant we define $a_k=F(n+1,k)-F(n,k)$. Thereafter we use Gosper's algorithm to find a $S_k$ such that
\begin{equation}
  a_k = S_k-S_{k-1}.
\end{equation}
Once we have that, we let $G(n,k) = S_{k-1}$ and we have obtained our function $G(n,k)$.

The steps of Gosper's algorithm are:
\begin{enumerate}
  \item Finding polynomials $p_k,q_k,r_k$ such that
  \begin{equation}\label{Eq: Theory,Gosper,step1}
    \frac{a_k}{a_{k-1}} = \frac{p_k}{p_{k-1}}\frac{q_k}{r_k}
  \end{equation}
  and $\gcd(q_k,r_{k+j})=1, \forall j\geq 0$.
  \item Finding polynomial $f_k$ such that
  \begin{equation}\label{Eq: Theory,Gosper,step2}
    p_k=q_{k+1}f_k-r_kf_{k-1}.
  \end{equation}
  \item Let
  \begin{equation}\label{Eq: Theory,Gosper,S}
    S_k=\frac{q_{k+1}}{p_k}f_ka_k.
  \end{equation}
\end{enumerate}
The reason this algorithm will provide a solution is that with $S_k$ given by \ref{Eq: Theory,Gosper,S} we have
\begin{equation}\label{Eq: Theory,Gosper,sksk1}
  S_k-S_{k-1} = \frac{q_{k+1}}{p_k}f_ka_k-\frac{q_{k}}{p_{k-1}}f_{k-1}a_{k-1}.
\end{equation}
By factoring out $\frac{a_k}{p_k}$ in \ref{Eq: Theory,Gosper,sksk1}, and using \ref{Eq: Theory,Gosper,step1} followed by \ref{Eq: Theory,Gosper,step2} gives us:
\begin{equation}
  \begin{split}
    S_k-S_{k-1} & = \frac{a_k}{p_k}\Big(q_{k+1}f_k-\frac{q_k}{p_{k-1}}f_{k-1}p_k\frac{a_{k-1}}{a_k}\Big) = \\
    & =\frac{a_k}{p_k}\Big(q_{k+1}f_k-\frac{q_k}{p_{k-1}}f_{k-1}p_k\frac{p_{k-1}}{p_k}\frac{r_k}{q_k}\Big)= \\
    & = \frac{a_k}{p_k}\Big(q_{k+1}f_k-r_kf_{k-1}\Big) = \frac{a_k}{p_k}p_k = a_k,
  \end{split}
\end{equation}
which is exactly what we wanted to be true for $S_k$.

Now, we have quite a few details to derive in each step -- both how the step is done more precisely and furthermore explaining why all the polynomials actually need to be polynomials and not rational polynomials instead.

\subsection{How to get polynomials $p,q,r$ in equation \ref{Eq: Theory,Gosper,step1}}
First of all, we need to prove that it is possible to find polynomials $p,q,r$ such that \ref{Eq: Theory,Gosper,step1} is fulfilled. Since $\frac{S_k}{S_{k-1}}$ is a rational function of $k$, then
\begin{equation}
  \frac{a_k}{a_{k-1}}=\frac{S_k-S_{k-1}}{S_{k-1}-S_{k-2}}=\frac{\frac{S_k}{S_{k-1}}-1}{1-\frac{S_{k-2}}{S_{k-1}}}
\end{equation}
is a rational function of $k$ as well. Therefore we will be able to find polynomials such that \ref{Eq: Theory,Gosper,step1} is fulfilled. Left is to show how to find polynomials such that $gcd(q_k,r_{k+j})=1 \forall j\geq 1$ as well.

We do this in a series of steps. First we let $p_k=1$ and $q_k, r_k$ be the numerator and denominator of $\frac{a_k}{a_k-1}$, respectively. Then if $gcd(q_k,r_{k+j})=1, \forall j\geq 0$ we are done. Otherwise we replace the polynomials by
\begin{equation}
  \begin{split}
    q_k^\prime & \leftarrow \frac{q_k}{g_k}, \\
    r_k^\prime & \leftarrow \frac{r_k}{g_{k-j}}, \\
    p_k^\prime & \leftarrow p_kg_kg_{k-1}\ldots g_{k-j+1},
  \end{split}
\end{equation}
where $g_k=gcd(q_k,r_{k+j})$ for the smallest $j\geq 0$ such that $gcd(q_k,r_{k+j})\neq 1$. Then we can see that we still have that
\begin{equation}
  \frac{a_k}{a_{k-1}} = \frac{p_k^\prime}{p_{k-1}^\prime}\frac{q_k^\prime}{r_k^\prime}
\end{equation}
and the degree of the polynomials $q_k, r_k$ are smaller. We then iterate this procedure as long as there exists a $j$ such that $gcd(q_k,r_{k+j})>1$. Then when this procedure finishes, we will obtain $q_k,r_k,p_k$ such that \ref{Eq: Theory,Gosper,step1} is fulfilled and $gcd(q_k,r_{k+j})=1 \forall j\geq 0$.

\subsection{How to get polynomial $f$ in equation \ref{Eq: Theory,Gosper,step2}}
The proof provided here is just due to Gosper's paper and mostly very similar. First of all, we need to prove that $f$ is a polynomial and not a rational polynomial.
\begin{proof}
  Assume that $f_k=\frac{c_k}{d_k}$ where $gcd(c_k,d_k)=1$. Then by plugging this into \ref{Eq: Theory,Gosper,step2} and multiplying by $d_kd_{k-1}$ gives us:
  \begin{equation}\label{Eq: Theory,8prime}
    d_kd_{k-1}p_k = c_kd_{k-1}q_{k+1} - d_kc_{k-1}r_k.
  \end{equation}
  Let $j$ be the largest integer such that
  \begin{equation}\label{Eq: Theory,10a}
    gcd(d_k,d_{k+j})=g_k\neq 1
  \end{equation}
  Since $g_k|d_{k+j}$ we get that
  \begin{equation}\label{Eq: Theory,10b}
    gcd(d_{k-1},d_{k+j})=1=gcd(d_{k-1},g_{k+j}).
  \end{equation}
  By just shifting $k$ by $-j-1$ in \ref{Eq: Theory,10a} we get that
  \begin{equation}\label{Eq: Theory,10c}
    gcd(d_{k-j-1},d_{k-1})=g_{k-j-1}\neq 1
  \end{equation}
  By shifting $k$ by $j$ in \ref{Eq: Theory,10b} we get that
  \begin{equation}\label{Eq: Theory,10d}
    gcd(d_{k-j-1},d_k) = 1 = gcd(g_{k-j-1},d_k),
  \end{equation}
  again since $g_{k-j-1}|d_{k-j-1}$.

  Now we consider equation \ref{Eq: Theory,8prime} upon dividing by first $g_k$ and then $g_{k-j-1}$. Clearly $g_k|d_kd_{k-1}p_k$, since $g_k|d_k$. This means that
  \begin{equation}
    g_k|c_kd_{k-1}q_{k+1}-d_kc_{k-1}r_k.
  \end{equation}
  Furthermore $g_k|d_kc_{k-1}r_k$ since $g_k|d_k$, which gives us that
  \begin{equation}
    g_k|c_kd_{k-1}q_{k+1}.
  \end{equation}
  By equation \ref{Eq: Theory,10b} we get $gcd(d_{k-1},g_k)=1$ and since $g_k|d_k$ and $gcd(c_k,d_k)=1$ we get $gcd(c_k,g_k)=1$. This means that
  \begin{equation}
    g_k|q_{k+1} \implies g_{k-1}|q_k.
  \end{equation}

  Similarly $g_{k-j-1}|d_kd_{k-1}p_k$, since $g_{k-j-1}|d_{k-1}$. This means that
  \begin{equation}
    g_{k-j-1}|c_kd_{k-1}q_{k+1}-d_kc_{k-1}r_k.
  \end{equation}
  Furthermore $g_{k-j-1}|c_kd_{k-1}q_{k+1}$ since $g_{k-j-1}|d_{k-1}$, which gives us that
  \begin{equation}
    g_k|d_kc_{k-1}r_k.
  \end{equation}
  By equation \ref{Eq: Theory,10d} we get $gcd(d_k,g_{k-j-1})=1$ and by $gcd(c_{k-1},d_{k-1})=1$ together with $g_{k-j-1}|d_{k-1}$ we get $gcd(c_{k-1},g_{k-j-1})=1$. This means that
  \begin{equation}
    g_{k-j-1}|r_k \implies g_{k-1}|r_{k+j}.
  \end{equation}
  But now we have $g_{k-1}$ divides both $r_{k+j}$ where $j\geq 0$ and $q_k$. From the first step of Gosper's algorithm we know that $gcd(r_{k+j},q_k)=1, \forall j\geq 0$ meaning that $g_k=1$. This means that for all $j\geq 0$ we have that
  \begin{equation}
    gcd(d_k,d_{k+j})=1.
  \end{equation}
  By putting $j=0$ we get that
  \begin{equation}
    d_k = gcd(d_k,d_k) = 1.
  \end{equation}
  Hence $d_k=1$ for all $k$ and thus $f_k$ is a polynomial.
\end{proof}

Now, let us derive how to get the polynomial $f_k$ such that \ref{Eq: Theory,Gosper,step2} is fulfilled. It is possible to get a bound for the degree of $f_k$ with respect to $k$. This is derived in the paper by Gosper (1978) and for the interested reader the details can be found there. The results is that
\begin{equation}
  deg(f_k)\leq deg(p_k) - max(deg(q_{k+1}+r_k),deg(q_{k+1}-r_k)) + 1.
\end{equation}
In some cases the degree can be further limited (without the extra $+1$) but we are mostly interested in that we have a bound -- not the extra added (small) constant.

We will now firstly show that there is a way to get the polynomial $f_k$ given that the degree of $f_k$ is less than $N$ for all variables in $f_k$. This is done by assigning
\begin{equation}\label{Eq: Theory,general polynomial}
  f_k = \sum_{\substack{\vec{i}\in (0,1,\ldots,N)^{m-1}\\i_1\in (0,1,\ldots,N)}} \Big(a_{\vec{i},i_1}k^{i_1}\prod_{j=2}^m v_j^{i_j}\Big),
\end{equation}
where $m$ is the number of variables in $f_k$, the variables of $f_k$ are named $v_j$ for $1\leq j\leq m$ where $v_1=k$, $a_\vec{i}$ are the coefficients and $\vec{i}=(i_2,i_3,\ldots,i_m)$ goes through all combinations of degrees in the range $0\leq deg(v_j)=i_j\leq N$ for all the different variables. The number $i_1$ is the exponent of $k$ and also goes through all the values for in the range $0\leq i_1\leq N$. This exponent is treaded slightly different just because it is the exponent of $k$.

Then from this we can derive what the polynomial $f_{k-1}$ is. First we write the polynomial $f_{k-1}$ on the same form as in \ref{Eq: Theory,general polynomial} but with other coefficients:
\begin{equation}\label{Eq: Theory,general polynomial k-1}
  f_{k-1} = \sum_{\vec{i}\in (0,1,\ldots,N)^m} \Big(b_{\vec{i},i_1}k^{i_1}\prod_{j=2}^m v_j^{i_j}\Big).
\end{equation}
Now we will derive the expression for $b_{\vec{i},i_1}$ in terms of $a_{\vec{i},i_1}$. If we replace $k$ by $k-1$ in \ref{Eq: Theory,general polynomial} we get
\begin{equation}\label{Eq: Theory,general polynomial k-1 plugged in}
  \begin{split}
    f_k & = \sum_{\substack{\vec{i}\in (0,1,\ldots,N)^{m-1}\\i_1\in (0,1,\ldots,N)}} \Big(a_{\vec{i},i_1}(k-1)^{i_1}\prod_{j=2}^m v_j^{i_j}\Big) = \\
    & = \sum_{\substack{\vec{i}\in (0,1,\ldots,N)^{m-1}\\i_1\in (0,1,\ldots,N)}} \Big(a_{\vec{i},i_1}\Bigg(\sum_{u=0}^{i_1} \binom{i_1}{u}k^u (-1)^{i_1-u}\Bigg)\prod_{j=2}^m v_j^{i_j}\Big) = \\
    & = \sum_{\substack{\vec{i}\in (0,1,\ldots,N)^{m-1}\\i_1\in (0,1,\ldots,N)}} \Bigg(\sum_{u=0}^{i_1} a_{\vec{i},i_1}\binom{i_1}{u}(-1)^{i_1-u}k^u\Bigg) \prod_{j=2}^m v_j^{i_j}
  \end{split}
\end{equation}
Now we can see that the term $k^{i_1^\prime}\prod_{j=2}^m v_j^{i_j}$ from \ref{Eq: Theory,general polynomial k-1} ($i_1^\prime$ denotes a specific $i_1$) appears in \ref{Eq: Theory,general polynomial k-1 plugged in} in several places -- namely when $i_1\in\{i_1^\prime,i_1^\prime+1,\ldots,N\}$. This means that we get
\begin{equation}\label{Eq: Theory,fk-1 coefficients}
  b_{\vec{i},i_1} = \sum_{s=i_1}^N a_{\vec{i},s}\binom{s}{i_1}(-1)^{s-i_1}
\end{equation}
Now we have all the background we need in order to set up the system of equations and then solve for $a_{\vec{i},i_1}$. This is done by noticing that the left and right hand sides of the equation
\begin{equation}\label{Eq: Theory,f equation}
  p_k=q_{k+1}f_k - r_k f_{k-1}
\end{equation}
In order to ease the formulation of the equation system, let $\vec{\hat{i}}=(\vec{i},i_1)$, let $\ll$ denote that a vector is pointwise smaller than or equal to e.g. $(1,2,3)\ll(2,3,3)$ and let $\vec{i}-\vec{j}$ denote the pointwise difference between $\vec{i}$ and $\vec{j}$, e.g. $(2,3,3)-(1,2,3)=(1,1,0)$. Finally we also denote the coefficients in front of $k^{i_1}\prod_{j=2}^N v_j^{i_j}$ in the polynomials $p_k, q_{k+1}, r_k$ by $r_\vec{\hat{i}}, q_\vec{\hat{i}}, r_\vec{\hat{i}}$, respectively. This gives us that
\begin{equation}\label{Eq: Theory,equation system1}
  p_\vec{\hat{i}} = \sum_{\substack{\vec{\hat{j}}\\\vec{\hat{j}}\ll\vec{\hat{i}}\\\vec{\hat{j^\prime}}=\vec{\hat{i}}-\vec{\hat{j}}}} a_{\vec{\hat{j}}}q_{\vec{\hat{j^\prime}}} - \sum_{\substack{\vec{\hat{j}}\\\vec{\hat{j}}\ll\vec{\hat{i}}\\\vec{\hat{j^\prime}}=\vec{\hat{i}}-\vec{\hat{j}}}} b_{\vec{\hat{j}}}r_{\vec{\hat{j^\prime}}},
\end{equation}
where $b_{\vec{\hat{j}}}$ is given by \ref{Eq: Theory,fk-1 coefficients}. Note that at this point in the algorithm all the numbers $p_\vec{\hat{i}},q_\vec{\hat{i}},r_\vec{\hat{i}}$ are simply constants. This means that we can formulate a system of equations
\begin{equation}\label{Eq: Theory,equation system2}
  Ma=P,
\end{equation}
where $M$ is the matrix that is given by the right hand side of \ref{Eq: Theory,equation system1}, $a$ consists of all coefficients $a_{\vec{\hat{j}}}$ and $P$ is the left hand side of \ref{Eq: Theory,equation system1}. Note that each row in this system of equations correspond to a different $\vec{\hat{i}}$. Here we go through all $\vec{\hat{i}}$ that are relevant -- meaning all $\vec{\hat{i}}$ with degree smaller than what either of the sides of equation \ref{Eq: Theory,equation system1} can have. This means that we have an overdetermined system of equations. Furthermore we demand that all the coefficients of $f_k$ have to be integers. This means that it is not certain that we will get a solution, but in case we do we have found $f_k$. If we do not get a solution that can be because of two different reasons:
\begin{enumerate}[i)]
  \item We chose a too small maximal degree $N$ for the polynomial $f_k$.
  \item There does not exist a solution, hence the identity cannot be shown by Wilf-Zeilbergers method.
\end{enumerate}
In the paper by Gosper (1978) the first reason for not finding a $f_k$ does not exists, since the paper proves an upper limit on the degree of $f_k$ with respect to $k$. This cannot be done for other variables, for instance with $p_k=q_{k+1}=r_k=1$ any polynomial $f_k$ on the form $f_k=n^c+k$ satisfies \ref{Eq: Theory,f equation}. \todo[inline]{Is it possible to show if there exists a solution there exist one with limited degree}

In the system \ref{Eq: Theory,equation system2}, $M$ can be constructed to have dimension $L\times (N^m)$, where $L=max(deg(p_k)+1,max(deg(q_{k+1}),deg(r_k))+N+1)$. Here $deg(g)$ denotes the largest degree in any variable of the polynomial $g$. The reason for this limit is that the row with highest degree for any variable can be chosen to be $L$, since the degree of either side of the equation \ref{Eq: Theory,equation system1} is at most $L$.

The time complexity for this system of equations is $\mathcal{O}(L\cdot N^{2m})$, since we perform the following algorithm:
\begin{algorithm}[H]
  \caption{Gaussian Elimination}
  \inp{Matrix $M$, vector $P$}
  \outp{Vector $x$ such that $Mx=P$}
  \begin{algorithmic}[1]
    \Procedure{GaussianElimination}{$M,P$}
      \State $r\gets \text{Number of rows in M}$
      \State $c\gets \text{Number of columns in M}$
      \For{$row\gets1,\ldots,c$}
        \State $i\gets $ first row with row number $\geq row $ and $M[i][c]\neq 0$
        \If{No such $i$ exists}
          \State Continue loop
        \EndIf
        \State $swap(M[row],M[i])$ \label{Extra linear term, start}
        \State $swap(P[row],P[i])$
        \For{$row_2\gets row+1,\ldots,L$}
          \State $g\gets gcd(M[row][c],M[row_2][c])$
          \State $M[row_2] \gets M[row_2]\cdot (M[row][c]/g) - M[row]\cdot (M[row_2][c]/g)$
          \State $P[row_2] \gets P[row_2]\cdot (M[row][c]/g) - P[row]\cdot (M[row_2][c]/g)$ \label{Extra linear term, end}
        \EndFor
      \EndFor
    \EndProcedure
  \end{algorithmic}
\end{algorithm}
\begin{remark}
  In the code we multiply a row vector by an integer. This means that we perform elementwise multiplication with this integer.
\end{remark}
Here we can clearly see that we have two loops, which are of size $c=N^m$ and $r=L$, respectively. The reason for the last $N^m$-factor is that when we perform row \ref{Extra linear term, start}--\ref{Extra linear term, end} we do this elementwise, hence once for each column.

This means that the time complexity is exponential in the number of variables. In general exponential time complexities are not desired at all -- since they grow very fast. In this thesis however we are working with identities that just involve a few (less than 5) different variables. Therefore this time complexity is acceptable. This means that given that given the limit on $m$ we have a polynomial time complexity for obtaining $f_k$.
